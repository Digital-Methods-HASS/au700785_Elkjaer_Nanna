---
title: "Preservation of buildings in Denmark"
author: "Nanna Elkjær-Larsen"
date: "2022-12-06"
output:
  pdf_document: default
  html_document: default
---

# Setting up RStudio 

Firstly I will install the necessary packages. 

```{r setup, echo=T, results='hide', message=FALSE}

knitr::opts_chunk$set(echo = TRUE)

#install.packages("leaflet")
#install.packages("tidyverse")
#install.packages("ggplot")

```

Then i will activate the packages in my library. 

```{r, echo=T, results='hide', message=FALSE}

library(leaflet)
library(tidyverse)
library(ggplot2)
```

# Creating my dataset 

The dataset which I use for this project is a list of the addresses of preservation cases in Denmark. The dataset is extracted from Kulturstyrelsens database and downloaded from their database as a pdf file. I wanted to be able to work with the data in a spreadsheet with different information separated into different columns. In order to separate the information into different columns I had to use the "text to columns" tool i excel. The "text to columns" tool allows me to divide the sentences into different columns separated by punctuation marks. In order to do this, I first had to clean the pdf from any unproductive symbols. I converted the pdf file to a word document and used the replace tool to edit out any unproductive punctuation marks e.g. "tidl.", "beg." as well as new lines.  

After having removed any unproductive symbols, I converted the word file to an excel file. This gave me each preservation case in separate rows, but in one column. I used the "text to columns" tool and separated the text into the columns: "adress", "name", "description", "preservation_date" and "remark." I then manually had to do the final tidying. I fixed any displacements and manually added "key_id" and "council". 

Once my dataset was tidied in excel, I needed to load the coordinates from google maps. This can be done manually by looking up the adress and copying the coordinates from the URL. Seeing as I had 3861 adresses it would be more productive to have excel pull the coordinates from google maps. In order to pull the coordinates from google, I used the VBA code of the GetCoordinates, GetLatitude, and GetLongitude functions by Christos Samaras, the code is in my github repository. I inserted this code in excel under "Developer", "Visual Basic" and made a new module. I followed the instructions of Christos Samaras and ran the code. 

After having run the code, I was then able to use the function "GetLatitude" and "GetLongtitude" which pulls the coordinates from google. The coordinates were placed in the column: "lat_raw" and "lon_raw". The number was not written with the correct decimal separator and I therefore had to divide the number by 10.000.000. This new number with the correct decimal seoperator was written in the column: "lat_tidy" and "lon_tidy."

This is the dataset which I am now able to work with in RStudio. 


# Working with data 

I start of reading my dataset into RStudio: 

```{r, echo=T, results='hide'}
read.csv("data/Fredningsliste_tidy.csv")
adress <- read.csv("data/Fredningsliste_tidy.csv")
```

## Inspecting and correcting dataframe 

After having loaded my dataframe and created an object "adress", I will firstly inspect my dataframe to ensure it is the correct spreadsheet and correct anything which might have been mistakenly included. In order to inspect my dataframe I use the function `head()` and `tail()` as they show me the first and last part of my dataframe. 

````{r}

head(adress)
tail(adress)

````

This shows that I have at least six rows with no answer. I will need to be aware of this in the following steps. 

I am now ready to create my visualizations. 

## Visualization 

In order to create a map I use the leaflet package which allows me to create an interactive map. The following steps describe how the interactive map is created: 

`leaflet()`: Creates an interactive map where it is possible to insert coordinates. 

`setView()`: Determines the centre of the view by inserting certain coordinates, as well as the zoom. In my case i choose the coordinates for Vig on Sjælland, as this i approximately the middle of Denmark. 

`addTiles()`: Adds the graphical element and layer to the map. 

`addMarkers()`: Adds pins to the map. The pins are placed according to coordinates consisting of latitude and longitude. These coordinates are found in my dataframe, latitude is lat_tidy and longitude is lon_tidy. Furthermore the pins can also contain information in a popup which is also defined here showing the "name", "preservation_date" and "description." Finally a clustering tool is added to make the map more manageable. 

`addProviderTiles()`: Customizes the map. I chose the Esri map as I find it to be clear and consistent.

`addMiniMap()`: Inserts a mini map which makes it easier to manage where in Denmark each preserved building is located, even when zooming in. 

```{r}

map_preservation <- leaflet() %>% 
  setView(11.5422344,55.8458097, zoom = 7) %>% 
  addTiles() %>% 
  addMarkers(lng = adress$lon_tidy, 
             lat = adress$lat_tidy,
             popup = paste('<b>',adress$name,'</b>','<br>',"Adress:",adress$council,",",adress$adress,'<br>',"Preserved:",adress$preservation_date,'<br>',"Details:",'<em>',adress$description,'</em>'),
             clusterOptions = markerClusterOptions()) %>% 
  addProviderTiles("Esri") %>% 
  addMiniMap(tiles = "Esri"[[1]], toggleDisplay = TRUE,
             position = "topright") 


map_preservation

```

The final map then shows preservation cases throughout Denmark. If you click a specific building, the map shows information on the specific building, the name, the adress, when it was preserved and details on the preservation case. 

# Trends of preservation in Denmark 

In order to analyse trends of preservation further, I would also like to do some statistics. I want to do a chart on the preservation dates in order to examine when preservation was most frequent in time. I would also like to do statistics on which council has the most preserved buildings i Denmark. 

## Preservations dates 

In order the get an insight into which years most preservation cases were finalized I use the `table()` function. This function allows we to see how many times a year occurs. 


```{r}

table(adress['preservation_date'])

```
 
This gives my an idea of the answer but makes it difficult to compare the different years. 
 
I furthermore want to inspect the data to see if I have any na values and inspect them if so. I use the `filter()` function and search specifically for na values. 

 
```{r}
adress %>% 
  filter(is.na(preservation_date))
```
I then see that I have 15 NA values. I inspect them and make sure that no data is missing. In the case of Rise Kirkelade, I can see in my raw data, that the preservation date is missing. After having had these reflections, I will use the `drop_na` function, although it is a bit drastic I am aware of the data which will be overlooked and can positively say that it won't interfere with my statistics. 
 
### Visualization 

I will then my a chart visualization. I use the `ggplot()` package and make a `geom_bar()` chart showing the frequency of preservation cases throughout time.  


```{r}
adress %>% 
  drop_na(preservation_date) %>% 
  mutate(preservation_date = as.numeric(preservation_date)) %>% 
  ggplot(aes(x=preservation_date))+
  geom_bar(stat="count")+
  theme(legend.position = "none")+
  coord_flip()+
  labs(title ="Frequency of preservation cases throughout time", subtitle = "When were buildings in Denmark preserved?", x="Preservation date", y="Frequency")
```

The chart above shows the frequency of preservation cases throughout time. 

## Frequency of preservation in councils  

In order to make a chart of frequency of preservation in councils I use the same method as described above. To get an overview I use the `table` function. The is as the example above hard to manage. 

```{r}
table(adress['council']) 

```

### Visualization

Using the `ggplot()` and `geom_bar()` functions I create a chart like the one above.  

```{r}

ggplot(data = adress,aes(x=council))+
  geom_bar(stat="count")+
  theme(legend.position = "none")+
  coord_flip()+
  labs(title ="Amount of preservation cases in Danish councils", subtitle = "Where are preserved buildings more frequent?", x="Council", y="Amount")+
  theme(axis.text.y = element_text(angle = 14, size = 6))

```

The chart above shows frequency of preservation in each council. The chart clearly shows that København has the most preserved buildings in Denmark. In order to get at better visualization of the amount of preserved buildings in the other councils I remove "København Kommune" from the chart. I do this by using the `subset()` function.

See

```{r}

ggplot(data = subset(adress, council != "København Kommune"),aes(x=council))+
  geom_bar(stat="count")+
  theme(legend.position = "none")+
  coord_flip()+
  labs(title ="Amount of preservation cases in Danish councils excl. København", subtitle = "Where are preserved buildings more frequent excl. København?", x="Council", y="Amount")+
  theme(axis.text.y = element_text(angle = 14, size = 6))


```

This chart is a little dense as it contains a lot of information. I thought of making seperate charts, one for Jylland, Fyn and København, but ultimately decided that wouldn't be productive for the questions I seek to answer. I have uploaded a picture to the "Finale project" folder which shows the different councils clearer to my github repository: 

https://raw.githubusercontent.com/Digital-Methods-HASS/au700785_Elkjaer_Nanna/main/Finale%20project/Frequency%20of%20preservation%20cases%20in%20Danish%20councils.png 

All of my visualizations can be found in my github repository: 

https://github.com/Digital-Methods-HASS/au700785_Elkjaer_Nanna 

The information on my RStudio session is described below. 

```{r}
sessionInfo()
```

